{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Input, MaxPooling2D, Dropout, Flatten\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 400\n",
    "img_width = 400\n",
    "batch_size = 32\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.50)\n",
    "\n",
    "train_generator = datagen.flow_from_directory('data/train',\n",
    "                                                    target_size=(img_height, img_width),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical')\n",
    "\n",
    "valid_generator = datagen.flow_from_directory('data/test',\n",
    "                                                    target_size=(img_height, img_width),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    subset='training')\n",
    "\n",
    "test_generator = datagen.flow_from_directory('data/test',\n",
    "                                                    target_size=(img_height, img_width),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    subset='validation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pretrained된 VGG16모델을 가져온다.\n",
    "# https://keras.io/api/applications/vgg/#vgg16-function\n",
    "pretrained_model = tf.keras.applications.VGG16(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    input_shape=(img_height, img_width, 3),\n",
    "    pooling=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model.trainable = False    # VGG16의 가중치들은 학습이 이루어지지 않도록 한다\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# CAM을 위해 하단에 다른 레이어를 추가해야하기 때문에 VGG16의 맨 마지막 레이어(pooling layer)를 제외하고 나머지 레이어를 model에 추가해준다\n",
    "for layer in pretrained_model.layers[:-1]:\n",
    "    model.add(layer)\n",
    "\n",
    "# CAM을 구현하기 위해 Conv2D, GlobalAveragePolling2D, Dense 레이어를 추가해준다\n",
    "model.add(tf.keras.layers.Conv2D(filters=1024, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "metrics = [keras.metrics.TruePositives(name='tp'),\n",
    "           keras.metrics.FalsePositives(name='fp'),\n",
    "           keras.metrics.TrueNegatives(name='tn'),\n",
    "           keras.metrics.FalseNegatives(name='fn'),\n",
    "           keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "           keras.metrics.Precision(name='precision'),\n",
    "           keras.metrics.Recall(name='recall'),\n",
    "           keras.metrics.AUC(name='auc')]\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "  train_generator,\n",
    "  steps_per_epoch=train_generator.samples//batch_size,\n",
    "  validation_data=valid_generator,\n",
    "  validation_steps=valid_generator.samples//batch_size,\n",
    "  epochs=epochs,\n",
    "  callbacks=[callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, loss_ax = plt.subplots()\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax.plot(history.history['accuracy'], 'b', label='train acc')\n",
    "acc_ax.plot(history.history['val_accuracy'], 'g', label='val acc')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| model | test_precision | test_recall | test_auc | test_accuracy |\n",
    "|---|\n",
    "| VGG16 | 0.9927 | 0.9964 | 0.9996 | 0.9937 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Hypermeters | Values |\n",
    "|---|\n",
    "| Epoch | 30 |\n",
    "| Bath Size | 32 |\n",
    "| Train Validation Test ratio | 8:1:1 |\n",
    "| Optimizer | Adam |\n",
    "| Input size | (400, 400, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  |  |\n",
    "|---|\n",
    "| TP=547 | FN=2 |\n",
    "| FP=4 | TN=400 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고\n",
    "# https://dryjelly.tistory.com/147\n",
    "# https://github.com/jacobgil/keras-cam/blob/master/cam.py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "\n",
    "get_output = tf.keras.backend.function([model.layers[0].input],\n",
    "                                       [model.layers[-3].output, model.layers[-1].output])    # 이미지 입력을 받아 model의 마지막에 추가한 Conv2D와 Dense 레이어를 출력하는 모델\n",
    "[conv_outputs, predictions] = get_output(np.array([valid_generator[0][0][0]]))                # 이미지 입력\n",
    "conv_outputs = conv_outputs[0, :, :, :]\n",
    "\n",
    "class_weights = model.layers[-1].get_weights()[0]    # model의 맨 마지막 레이어인 Dense 레이어의 weight를 가져온다\n",
    "\n",
    "cam = np.zeros(dtype = np.float32, shape = conv_outputs.shape[0:2])\n",
    "for i, w in enumerate(class_weights[:, 1]):\n",
    "        cam += w * conv_outputs[:, :, i]    # 마지막 레이어의 각 가중치와 Conv2D의 결과를 곱하여 누적하여 가산 -> CAM 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 이미지를 보기 좋게 가공\n",
    "\n",
    "# CAM 이미지의 값 범위를 0 부터 255까지로 맞추고\n",
    "# 이미지 크기를 원본 데이터와 일치하도록 resize한다\n",
    "heatmap = cam\n",
    "heatmap = heatmap / (np.max(heatmap) - np.min(heatmap)) * 255.9\n",
    "heatmap = np.uint8(heatmap - np.min(heatmap))\n",
    "heatmap = cv2.resize(heatmap, (img_height, img_width))\n",
    "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "# 원본 이미지도 비슷하게 처리\n",
    "origin_img = valid_generator[0][0][0]\n",
    "origin_img = origin_img / (np.max(origin_img) - np.min(origin_img)) * 255.9\n",
    "origin_img = np.uint8(origin_img - np.min(origin_img))\n",
    "\n",
    "# 원본 이미지와 CAM 합성\n",
    "a = 0.6\n",
    "dst = cv2.addWeighted(origin_img, a, heatmap, 1 - a, 0)\n",
    "plt.imshow(dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2_p36] *",
   "language": "python",
   "name": "conda-env-tensorflow2_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
