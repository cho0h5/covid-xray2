{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 2 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# 이 셀은 무시하셔도 됩니다\n",
    "\n",
    "import tensorflow as tf\n",
    " \n",
    " \n",
    " \n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()    # mnist data import\n",
    "\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 19,466\n",
      "Trainable params: 19,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        # CAM을 구현하기 위해 Conv2D, GlobalMaxPooling2D, Dense 추가\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        keras.layers.GlobalMaxPooling2D(),\n",
    "        tf.keras.layers.Dense(10, activation='softmax'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.8407 - accuracy: 0.7848 - val_loss: 0.2527 - val_accuracy: 0.9260\n",
      "Epoch 2/15\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.2237 - accuracy: 0.9338 - val_loss: 0.1655 - val_accuracy: 0.9492\n",
      "Epoch 3/15\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.1652 - accuracy: 0.9486 - val_loss: 0.1345 - val_accuracy: 0.9605\n",
      "Epoch 4/15\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.1358 - accuracy: 0.9586 - val_loss: 0.1216 - val_accuracy: 0.9623\n",
      "Epoch 5/15\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.1172 - accuracy: 0.9631 - val_loss: 0.1097 - val_accuracy: 0.9653\n",
      "Epoch 6/15\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.1057 - accuracy: 0.9670 - val_loss: 0.0947 - val_accuracy: 0.9695\n",
      "Epoch 7/15\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.0960 - accuracy: 0.9694 - val_loss: 0.0880 - val_accuracy: 0.9717\n",
      "Epoch 8/15\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.0890 - accuracy: 0.9716 - val_loss: 0.0883 - val_accuracy: 0.9740\n",
      "Epoch 9/15\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.0824 - accuracy: 0.9741 - val_loss: 0.0915 - val_accuracy: 0.9713\n",
      "Epoch 10/15\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.0757 - accuracy: 0.9758 - val_loss: 0.0806 - val_accuracy: 0.9748\n",
      "Epoch 11/15\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.0724 - accuracy: 0.9770 - val_loss: 0.0776 - val_accuracy: 0.9760\n",
      "Epoch 12/15\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.0680 - accuracy: 0.9782 - val_loss: 0.0777 - val_accuracy: 0.9755\n",
      "Epoch 13/15\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.0633 - accuracy: 0.9792 - val_loss: 0.0735 - val_accuracy: 0.9778\n",
      "Epoch 14/15\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.0630 - accuracy: 0.9799 - val_loss: 0.0791 - val_accuracy: 0.9757\n",
      "Epoch 15/15\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.0567 - accuracy: 0.9819 - val_loss: 0.0769 - val_accuracy: 0.9758\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2cf01affd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.06933323293924332\n",
      "Test accuracy: 0.978600025177002\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고\n",
    "# https://dryjelly.tistory.com/147\n",
    "# https://github.com/jacobgil/keras-cam/blob/master/cam.py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "\n",
    "get_output = tf.keras.backend.function([model.layers[0].input],\n",
    "                                       [model.layers[-3].output, model.layers[-1].output])    # 이미지 입력을 받아 model의 마지막에 추가한 Conv2D와 Dense 레이어를 출력하는 모델\n",
    "[conv_outputs, predictions] = get_output(np.array([x_test[0]]))                # 이미지 입력\n",
    "conv_outputs = conv_outputs[0, :, :, :]\n",
    "\n",
    "class_weights = model.layers[-1].get_weights()[0]    # model의 맨 마지막 레이어인 Dense 레이어의 weight를 가져온다\n",
    "\n",
    "cam = np.zeros(dtype = np.float32, shape = conv_outputs.shape[0:2])\n",
    "for i, w in enumerate(class_weights[:, 1]):\n",
    "        cam += w * conv_outputs[:, :, i]    # 마지막 레이어의 각 가중치와 Conv2D의 결과를 곱하여 누적하여 가산 -> CAM 구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2c780a0f28>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 결과 이미지를 보기 좋게 가공\n",
    "\n",
    "# CAM 이미지의 값 범위를 0 부터 255까지로 맞추고\n",
    "# 이미지 크기를 원본 데이터와 일치하도록 resize한다\n",
    "heatmap = cam\n",
    "heatmap = heatmap / (np.max(heatmap) - np.min(heatmap)) * 255.9\n",
    "heatmap = np.uint8(heatmap - np.min(heatmap))\n",
    "heatmap = cv2.resize(heatmap, (28, 28))\n",
    "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "# 원본 이미지도 비슷하게 처리\n",
    "origin_img = np.reshape(x_test[0], (28, 28))\n",
    "origin_img = origin_img / (np.max(origin_img) - np.min(origin_img)) * 255.9\n",
    "origin_img = np.uint8(origin_img - np.min(origin_img))\n",
    "origin_img = cv2.merge((origin_img,origin_img,origin_img))\n",
    "\n",
    "# 원본 이미지와 CAM 합성\n",
    "a = 0.6\n",
    "dst = cv2.addWeighted(origin_img, a, heatmap, 1 - a, 0)\n",
    "plt.imshow(dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2_p36] *",
   "language": "python",
   "name": "conda-env-tensorflow2_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
